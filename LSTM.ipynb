{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'synthetic' #natural, original or synthetic\n",
    "if dataset == 'natural':\n",
    "    classes = pd.read_csv(\"augmented_natural_dataset\\elliptic_txs_classes.csv\")\n",
    "    features = pd.read_csv(\"augmented_natural_dataset\\elliptic_txs_features.csv\", header=None)\n",
    "elif dataset == 'original':\n",
    "    classes = pd.read_csv(\"elliptic_bitcoin_dataset\\elliptic_txs_classes.csv\")\n",
    "    features = pd.read_csv(\"elliptic_bitcoin_dataset\\elliptic_txs_features.csv\", header=None)\n",
    "else:\n",
    "    samples = pd.read_csv(\"augmented_synthetic_dataset\\synthetic_illicit_tx.csv\")\n",
    "    data = pd.read_csv(\"augmented_synthetic_dataset\\labelled_tx.csv\")\n",
    "    data.columns = samples.columns\n",
    "    frames = [data, samples]\n",
    "    result = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (dataset == 'natural' or dataset == 'original'):\n",
    "    display(features.head(5),classes.head(5))\n",
    "else:\n",
    "    display(result.head(5))\n",
    "    display(result.groupby('class').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (dataset == 'natural' or dataset == 'original'):\n",
    "    tx_features = [\"local_feat_\"+str(i) for i in range(2,95)]\n",
    "    agg_features = [\"agg_feat_\"+str(i) for i in range(1,73)]\n",
    "    features.columns = [\"txId\",\"time_step\"] + tx_features + agg_features\n",
    "    features = pd.merge(features,classes,left_on=\"txId\",right_on=\"txId\",how='left')\n",
    "    features['class'] = features['class'].apply(lambda x: '0' if x == \"unknown\" else x)\n",
    "else:\n",
    "    local_features = [\"Local_feature_\"+str(i) for i in range(1,94)]\n",
    "    agg_features = [\"Aggregate_feature_\"+str(i) for i in range(1,73)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (dataset == 'natural' or dataset == 'original'):\n",
    "    features = features.drop(columns=['txId', 'time_step'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (dataset == 'natural' or dataset == 'original'):\n",
    "    features.groupby('class').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (dataset == 'natural'):\n",
    "    features = features.replace(\"suspicious\", \"1\")\n",
    "    features.groupby('class').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (dataset == 'natural' or dataset == 'original'):\n",
    "    data = features[(features['class']=='1') | (features['class']=='2')] #We remove unknown transactions from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (dataset == 'natural' or dataset == 'original'):\n",
    "    data.groupby('class').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (dataset == 'natural' or dataset == 'original'):\n",
    "    X = data[tx_features + agg_features]\n",
    "    y = data['class']\n",
    "    y = y.apply(lambda x: 0 if x == '2' else 1 )\n",
    "else: \n",
    "    X = result[local_features + agg_features]\n",
    "    y = result['class']\n",
    "    y = y.apply(lambda x: 0 if x == 2 else 1 )\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=30, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_array = X_train.to_numpy()\n",
    "X_test_array = X_test.to_numpy()\n",
    "y_train_array = y_train.to_numpy()\n",
    "y_test_array = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train_array = np.reshape(X_train_array, (X_train_array.shape[0], 1, X_train_array.shape[1]))\n",
    "X_test_array = np.reshape(X_test_array, (X_test_array.shape[0], 1, X_test_array.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_array.shape)\n",
    "print(X_test_array.shape)\n",
    "print(y_train_array.shape)\n",
    "print(y_test_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(166, input_shape=(1, 165), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.add(Flatten())\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1_m,precision_m, recall_m])\n",
    "model.fit(X_train_array, y_train_array, validation_data=(X_test_array, y_test_array), epochs=1000, batch_size=32)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "f1_score_list = list()\n",
    "precision_list = list()\n",
    "recall_list = list()\n",
    "for r in range(10):\n",
    "    loss, f1_score, precision, recall = model.evaluate(X_test_array, y_test_array, verbose=0)\n",
    "    f1_score = f1_score * 100.0\n",
    "    precision = precision * 100.0\n",
    "    recall = recall * 100.0\n",
    "    f1_score_list.append(f1_score)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "\n",
    "import statistics\n",
    "\n",
    "print(statistics.mean(precision_list))\n",
    "print(statistics.mean(recall_list))\n",
    "print(statistics.mean(f1_score_list))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ae5b0e5206a6f34ff062d2dc505c70cc0f209158cf46fb72598ab1e7b8e104b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
